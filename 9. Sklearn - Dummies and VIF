# Import the relevant libraries
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import seaborn as sns
sns.set()

# Load data from external csv file and view data
raw_data = pd.read_csv('1.04. Real-life example.csv')
print(raw_data.head())

# Explore the descriptive statistics of the features and determine the features of interest
print(raw_data.describe(include='all'))
data = raw_data.drop(['Model'],axis=1)
print(data.describe(include='all'))

# Find missing values
print(data.isnull().sum())
data_no_mv = data.dropna(axis=0)
print(data_no_mv.describe(include='all'))

# Explore the probability distribution functions
sns.distplot(data_no_mv['Price'])

# Find out which of the independent features have outliers and exclude (within reason)
sns.distplot(data_no_mv['Price'])
q = data_no_mv['Price'].quantile(0.99)
data_1 = data_no_mv[data_no_mv['Price']<q]
print(data_1.describe(include='all'))

sns.distplot(data_1['Mileage'])
q = data_1['Mileage'].quantile(0.99)
data_2 = data_1[data_1['Mileage']<q]

sns.distplot(data_1['EngineV'])
data_3 = data_2[data_2['EngineV']<6.5]

sns.distplot(data_2['Year''])
q = data_3['Year'].quantile(0.01)
data_4 = data_3[data_3['Year']>q]

data_cleaned = data_4.reset_index(drop=True)

# View details for cleansed data
print(data_cleaned.describe(include='all'))

# Check data to ensure OLS assumptions are met - 1. Linear relationship
f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))
ax1.scatter(data_cleaned['Year'],data_cleaned['Price'])
ax1.set_title('Price and Year')
ax2.scatter(data_cleaned['EngineV'],data_cleaned['Price'])
ax2.set_title('Price and EngineV')
ax3.scatter(data_cleaned['Mileage'],data_cleaned['Price'])
ax3.set_title('Price and Mileage')

plt.show()
# Plots showed there was not a linear relationship between price and all independent features. A log transformation has been applied below, this assumption is now met.
log_price = np.log(data_cleaned['Price'])
data_cleaned['log_price'] = log_price
print(data_cleaned)

# Assumption 2. No multicollinearity (calculate the variance inflation factor)
print(data_cleaned.columns.values)
from statsmodels.stats.outliers_influence import variance_inflation_factor
variables = data_cleaned[['Mileage','Year','EngineV']]
vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif["features"] = variables.columns
print(vif)
# Year has high multicollinearity and therefore this feature has been dropped.
data_no_multicollinearity = data_cleaned.drop(['Year'],axis=1)

# Create dummy variables
data_with_dummies = pd.get_dummies(data_no_multicollinearity, drop_first=True)
print(data_with_dummies.head())

