# Import the relevant libraries
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

# Load and view the training data
raw_data = pd.read_csv('Bank-data.csv')
print(raw_data)

# Create a copy of the data, remove the index column and change the outcome variable into Boolean values
data = raw_data.copy()
data = data.drop(['Unnamed: 0'], axis = 1)
data['y'] = data['y'].map({'yes':1, 'no':0})
print(data)
print(data.describe())

# Declare the dependent and independent variables
y = data['y']
x1 = data['duration']

# Run the logistic regression
x = sm.add_constant(x1)
reg_log = sm.Logit(y,x)
results_log = reg_log.fit()
print(results_log.summary())

# Create a scatter pyplot
plt.scatter(x1,y,color = 'C0')
plt.xlabel('Duration', fontsize = 20)
plt.ylabel('Subscription', fontsize = 20)
print(plt.show())

# Expand the model so that all independent variables are included
estimators=['interest_rate','credit','march','previous','duration']
X1_all = data[estimators]
y = data['y']

# Run the logistic regression on the expanded model
X_all = sm.add_constant(X1_all)
reg_logit = sm.Logit(y,X_all)
results_logit = reg_logit.fit()
results_logit.summary2()

##Test the model
# Load the testing data
raw_data2 = pd.read_csv('Bank-data-testing.csv')

# Create a copy of the test data, remove the index column and change the outcome variable into Boolean values
data_test = raw_data2.copy()
data_test = data_test.drop(['Unnamed: 0'], axis = 1)
data_test['y'] = data_test['y'].map({'yes':1, 'no':0})
print(data_test)

# Declare the dependent and independent variables within the test data (same as the training data)
y_test = data_test['y']
X1_test = data_test[estimators]
X_test = sm.add_constant(X1_test)

# Determine the confusion matrix and the accuracy of the model with the training data
confusion_matrix(X_all,y, results_logit)

# Determine the confusion matrix and the accuracy of the model with the testing data
confusion_matrix(X_test, y_test, results_logit)

# The training data has an accuracy of 86.29%, the testing data has a slightly lower accuracy of 86.04%. In general, the testing accuracy is expected to be lower than the training accuracy and therefore the results are in line with our expectations.
